package golem:llm@1.0.0;

interface llm {
  // --- Roles, Error Codes, Finish Reasons ---

  enum role {
    user,
    assistant,
    system,
    tool,
  }

  enum error-code {
    invalid-request,
    authentication-failed,
    rate-limit-exceeded,
    internal-error,
    unsupported,
    unknown,
  }

  enum finish-reason {
    stop,
    length,
    tool-calls,
    content-filter,
    error,
    other,
  }

  enum image-detail {
    low,
    high,
    auto,
  }

  // --- Message Content ---

  record image-url {
    url: string,
    detail: option<image-detail>,
  }

  variant content-part {
    text(string),
    image(image-url),
  }

  record message {
    role: role,
    name: option<string>,
    content: list<content-part>,
  }

  // --- Tooling ---

  record tool-definition {
    name: string,
    description: option<string>,
    parameters-schema: string,
  }

  record tool-call {
    id: string,
    name: string,
    arguments-json: string,
  }

  record tool-success {
    id: string,
    name: string,
    result-json: string,
    execution-time-ms: option<u32>,
  }

  record tool-failure {
    id: string,
    name: string,
    error-message: string,
    error-code: option<string>,
  }

  variant tool-result {
    success(tool-success),
    error(tool-failure),
  }

  // --- Configuration ---

  record kv {
    key: string,
    value: string,
  }

  record config {
    model: string,
    temperature: option<f32>,
    max-tokens: option<u32>,
    stop-sequences: option<list<string>>,
    tools: list<tool-definition>,
    tool-choice: option<string>,
    provider-options: list<kv>,
  }

  // --- Usage / Metadata ---

  record usage {
    input-tokens: option<u32>,
    output-tokens: option<u32>,
    total-tokens: option<u32>,
  }

  record response-metadata {
    finish-reason: option<finish-reason>,
    usage: option<usage>,
    provider-id: option<string>,
    timestamp: option<string>,
    provider-metadata-json: option<string>,
  }

  record complete-response {
    id: string,
    content: list<content-part>,
    tool-calls: list<tool-call>,
    metadata: response-metadata,
  }

  // --- Error Handling ---

  record error {
    code: error-code,
    message: string,
    provider-error-json: option<string>,
  }

  // --- Chat Response Variants ---

  variant chat-event {
    message(complete-response),
    tool-request(list<tool-call>),
    error(error),
  }

  // --- Streaming ---

  record stream-delta {
    content: option<list<content-part>>,
    tool-calls: option<list<tool-call-delta>>,
  }

  // A tool-call delta is a partial update to a tool call
  record tool-call-delta {
    id: option<string>,
    name: option<string>,
    arguments-json: string,
  }

  variant stream-event {
    delta(stream-delta),
    finish(response-metadata),
    error(error),
  }

  resource chat-stream {
    get-next: func () -> stream-event;
    has-next: func () -> bool;
  }

  // --- Core Functions ---

  send: func (
    messages: list<message>,
    config: config
  ) -> result<chat-event, error>;

  continue-inference: func (
    messages: list<message>,
    tool-results: list<tool-result>,
    config: config
  ) -> result<chat-event, error>;

  stream-chat: func (
    messages: list<message>,
    config: config
  ) -> result<chat-stream, error>;
}

world libllm {
  export llm;
}